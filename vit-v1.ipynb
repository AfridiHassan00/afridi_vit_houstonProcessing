{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12591615,"sourceType":"datasetVersion","datasetId":7952805}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required package first\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:43:18.655174Z","iopub.execute_input":"2025-07-28T07:43:18.655573Z","iopub.status.idle":"2025-07-28T07:43:21.865347Z","shell.execute_reply.started":"2025-07-28T07:43:18.655547Z","shell.execute_reply":"2025-07-28T07:43:21.864594Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\n\n# Define the base path to the dataset\ndata_path = \"/kaggle/input/houston2018-tensors-probably-skewed\"\n\n# Load the tensors\nX_train = torch.load(f\"{data_path}/X_train.pt\")\ny_train = torch.load(f\"{data_path}/y_train.pt\")\nX_test = torch.load(f\"{data_path}/X_test.pt\")  # Added this line\ny_test = torch.load(f\"{data_path}/y_test.pt\")\n\n# Verify shapes\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)   # Added this line\nprint(\"y_test shape:\", y_test.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:43:21.867037Z","iopub.execute_input":"2025-07-28T07:43:21.867261Z","iopub.status.idle":"2025-07-28T07:43:23.847599Z","shell.execute_reply.started":"2025-07-28T07:43:21.867238Z","shell.execute_reply":"2025-07-28T07:43:23.846950Z"}},"outputs":[{"name":"stdout","text":"X_train shape: torch.Size([11829, 50, 3, 3])\ny_train shape: torch.Size([11829])\nX_test shape: torch.Size([1312965, 50, 3, 3])\ny_test shape: torch.Size([1312965])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, embed_dim=256, dropout=0.1):\n        super().__init__()\n        self.num_tokens = 9  # 3x3 = 9 spatial positions\n        self.num_bands = 50  # Your data has 50 spectral bands\n        \n        # Linear projection for spectral dimension\n        self.projection = nn.Linear(self.num_bands, embed_dim)\n        \n        # Learnable positional embedding (+1 for CLS token)\n        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens + 1, embed_dim))\n        \n        # CLS token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.projection.weight)\n        if self.projection.bias is not None:\n            nn.init.zeros_(self.projection.bias)\n        nn.init.normal_(self.pos_embedding, std=0.02)\n        nn.init.normal_(self.cls_token, std=0.02)\n    \n    def forward(self, x):\n        # x shape: (batch_size, num_bands, 3, 3)\n        batch_size = x.shape[0]\n        \n        # Reshape patch into sequence of tokens\n        # (batch_size, num_bands, 3, 3) -> (batch_size, 9, num_bands)\n        x = x.reshape(batch_size, self.num_bands, 9).transpose(1, 2)\n        \n        # Project each token's spectral vector to embedding dimension\n        x = self.projection(x)  # (batch_size, 9, embed_dim)\n        \n        # Prepend CLS token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, 10, embed_dim)\n        \n        # Add positional embedding\n        x = x + self.pos_embedding\n        \n        return self.dropout(x)\n\nclass SpectralTransformer(nn.Module):\n    def __init__(\n        self,\n        embed_dim=256,\n        num_layers=6,\n        num_heads=8,\n        mlp_ratio=4,\n        dropout=0.1,\n        attention_dropout=0.1,\n    ):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(\n            embed_dim=embed_dim,\n            dropout=dropout\n        )\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=embed_dim * mlp_ratio,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True\n        )\n        \n        self.transformer = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=num_layers,\n            norm=nn.LayerNorm(embed_dim)\n        )\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.transformer(x)\n        x = self.norm(x)\n        return x[:, 0]\n\ndef create_model(\n    embed_dim=256,\n    num_layers=6,\n    num_heads=8,\n    mlp_ratio=4,\n    dropout=0.1,\n    attention_dropout=0.1,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n):\n    model = SpectralTransformer(\n        embed_dim=embed_dim,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        mlp_ratio=mlp_ratio,\n        dropout=dropout,\n        attention_dropout=attention_dropout\n    ).to(device)\n    return model\n\n# Test and verify the model\nif __name__ == \"__main__\":\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    try:\n        # Create a test batch\n        batch_size = 32\n        x = torch.randn(batch_size, 50, 3, 3).to(device)\n        print(f\"Created test input with shape: {x.shape}\")\n        \n        # Create model\n        model = create_model(\n            embed_dim=256,\n            num_layers=6,\n            num_heads=8,\n            device=device\n        )\n        print(\"Created model successfully\")\n        \n        # Forward pass\n        with torch.no_grad():\n            output = model(x)\n            print(f\"Forward pass successful!\")\n            print(f\"Input shape: {x.shape}\")\n            print(f\"Output shape: {output.shape}\")  # Should be (batch_size, embed_dim)\n        \n        # Optional: Test with actual data if available\n        if 'X_train' in globals():\n            print(\"\\nTesting with actual training data:\")\n            x_sample = X_train[:32].to(device)\n            with torch.no_grad():\n                output = model(x_sample)\n                print(f\"Real data input shape: {x_sample.shape}\")\n                print(f\"Real data output shape: {output.shape}\")\n    \n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:43:23.848449Z","iopub.execute_input":"2025-07-28T07:43:23.848722Z","iopub.status.idle":"2025-07-28T07:43:23.906525Z","shell.execute_reply.started":"2025-07-28T07:43:23.848695Z","shell.execute_reply":"2025-07-28T07:43:23.905892Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCreated test input with shape: torch.Size([32, 50, 3, 3])\nCreated model successfully\nForward pass successful!\nInput shape: torch.Size([32, 50, 3, 3])\nOutput shape: torch.Size([32, 256])\n\nTesting with actual training data:\nReal data input shape: torch.Size([32, 50, 3, 3])\nReal data output shape: torch.Size([32, 256])\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Install required package first\n!pip install transformers\n\n# Now import required libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\n\n# Define TextEncoder class with all class descriptions\nclass TextEncoder(nn.Module):\n    \"\"\"\n    Text encoder using BERT for encoding class names/descriptions into semantic feature vectors.\n    This forms the text encoder component of the vision-language system.\n    \"\"\"\n    def __init__(\n        self,\n        model_name: str = \"bert-base-uncased\",\n        embedding_dim: int = 256,\n        device: str = None,\n    ):\n        super().__init__()\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Load BERT model and tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.bert = AutoModel.from_pretrained(model_name).to(self.device)\n        \n        # Freeze BERT by default\n        for param in self.bert.parameters():\n            param.requires_grad = False\n            \n        # Project from BERT dimension (768) to desired embedding dimension\n        self.projection = nn.Linear(768, embedding_dim).to(self.device)\n        \n        # Class descriptions for Houston dataset\n        self.class_descriptions = {\n            \"Healthy grass\": (\n                \"A region showing vibrant, well-maintained grass vegetation with high chlorophyll content. \"\n                \"The spectral signature indicates optimal plant health and photosynthetic activity. \"\n                \"This class represents areas of grass that receive adequate water and nutrients.\"\n            ),\n            \"Stressed grass\": (\n                \"An area of grass vegetation showing signs of environmental stress or deterioration. \"\n                \"The spectral signature shows reduced chlorophyll content and photosynthetic activity. \"\n                \"This may be due to insufficient water, nutrient deficiency, or other environmental stressors.\"\n            ),\n            \"Artificial turf\": (\n                \"A synthetic surface designed to mimic natural grass, typically used in sports fields. \"\n                \"The spectral signature is highly uniform and lacks the natural variation of real vegetation. \"\n                \"This material shows consistent reflectance patterns across the spectrum.\"\n            ),\n            \"Evergreen trees\": (\n                \"Dense canopy of trees that maintain their foliage throughout the year. \"\n                \"The spectral signature shows strong absorption in visible bands and high reflectance in NIR. \"\n                \"These trees maintain consistent photosynthetic activity across seasons.\"\n            ),\n            \"Deciduous trees\": (\n                \"Trees that seasonally shed their leaves, showing variable spectral patterns. \"\n                \"The canopy structure and leaf characteristics affect the spectral signature. \"\n                \"These trees show seasonal variations in their spectral response.\"\n            ),\n            \"Bare earth\": (\n                \"Exposed soil surface without vegetation cover. \"\n                \"The spectral signature is influenced by soil composition, moisture, and organic content. \"\n                \"This class represents areas of bare ground or cleared land.\"\n            ),\n            \"Water\": (\n                \"Bodies of water including ponds, lakes, or other water features. \"\n                \"The spectral signature shows strong absorption in NIR and characteristic water absorption bands. \"\n                \"Water depth and quality influence the spectral response.\"\n            ),\n            \"Residential buildings\": (\n                \"Single-family homes and residential structures. \"\n                \"The spectral signature is influenced by roofing materials and urban features. \"\n                \"These areas show typical patterns of residential development.\"\n            ),\n            \"Non-residential buildings\": (\n                \"Commercial, industrial, or institutional buildings. \"\n                \"The spectral signature varies with building materials and roof types. \"\n                \"These structures often have larger footprints than residential buildings.\"\n            ),\n            \"Roads\": (\n                \"Paved transportation routes including streets and access roads. \"\n                \"The spectral signature is characteristic of asphalt or concrete surfaces. \"\n                \"These features show linear patterns in the urban landscape.\"\n            ),\n            \"Sidewalks\": (\n                \"Concrete or paved pedestrian walkways. \"\n                \"The spectral signature is typical of concrete or similar materials. \"\n                \"These features are usually adjacent to roads and buildings.\"\n            ),\n            \"Crosswalks\": (\n                \"Marked pedestrian crossing areas on roads. \"\n                \"The spectral signature shows patterns of road marking materials. \"\n                \"These features have distinctive patterns within road surfaces.\"\n            ),\n            \"Major thoroughfares\": (\n                \"Wide, main roads with multiple lanes for higher traffic volume. \"\n                \"The spectral signature indicates extensive paved surfaces. \"\n                \"These roads are major transportation arteries.\"\n            ),\n            \"Highways\": (\n                \"Large, multi-lane roads for high-speed vehicular traffic. \"\n                \"The spectral signature shows extensive asphalt or concrete surfaces. \"\n                \"These are the largest road features in the urban landscape.\"\n            ),\n            \"Railways\": (\n                \"Train tracks and associated railroad infrastructure. \"\n                \"The spectral signature includes tracks, gravel, and surrounding features. \"\n                \"These features show distinctive linear patterns.\"\n            ),\n            \"Paved parking lots\": (\n                \"Asphalt or concrete surfaces designated for vehicle parking. \"\n                \"The spectral signature is similar to roads but in larger continuous areas. \"\n                \"These areas show extensive impervious surface coverage.\"\n            ),\n            \"Unpaved parking lots\": (\n                \"Gravel or dirt surfaces used for vehicle parking. \"\n                \"The spectral signature indicates unpaved, compacted surfaces. \"\n                \"These areas show different patterns from paved surfaces.\"\n            ),\n            \"Cars\": (\n                \"Parked or moving vehicles visible in the image. \"\n                \"The spectral signature is influenced by vehicle materials and shadows. \"\n                \"These features appear as small objects in parking areas or on roads.\"\n            ),\n            \"Trains\": (\n                \"Railroad cars or locomotives on railway tracks. \"\n                \"The spectral signature includes metal surfaces and associated features. \"\n                \"These objects appear along railway infrastructure.\"\n            ),\n            \"Stadium seats\": (\n                \"Seating areas in sports stadiums or amphitheaters. \"\n                \"The spectral signature is influenced by seating materials and arrangement. \"\n                \"These features show regular patterns in recreational facilities.\"\n            )\n        }\n        \n    def encode_class_description(self, class_name: str) -> torch.Tensor:\n        \"\"\"\n        Encode a class description into a feature vector.\n        Uses full description if available, otherwise uses the class name.\n        \"\"\"\n        # Get full description if available, otherwise use class name\n        text = self.class_descriptions.get(class_name, class_name)\n        \n        # Tokenize with truncation and padding\n        inputs = self.tokenizer(\n            text,\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\"\n        ).to(self.device)\n        \n        # Get BERT embeddings\n        with torch.no_grad():\n            outputs = self.bert(**inputs)\n            # Use CLS token embedding as text representation\n            text_features = outputs.last_hidden_state[:, 0, :]\n        \n        # Project to desired dimension and normalize\n        text_features = self.projection(text_features)\n        text_features = torch.nn.functional.normalize(text_features, p=2, dim=-1)\n        \n        return text_features\n    \n    def encode_batch(self, class_names: list) -> torch.Tensor:\n        \"\"\"\n        Encode a batch of class names into feature vectors.\n        \"\"\"\n        embeddings = []\n        for class_name in class_names:\n            embedding = self.encode_class_description(class_name)\n            embeddings.append(embedding)\n        \n        return torch.cat(embeddings, dim=0)\n    \n    def forward(self, class_names: list) -> torch.Tensor:\n        \"\"\"\n        Forward pass - encode a batch of class names.\n        \"\"\"\n        return self.encode_batch(class_names)\n    \n    def unfreeze(self, lr_scale: float = 0.1):\n        \"\"\"Unfreeze BERT for fine-tuning with scaled learning rate.\"\"\"\n        for param in self.bert.parameters():\n            param.requires_grad = True\n            if hasattr(param, 'lr_scale'):\n                param.lr_scale = lr_scale\n                \n    def freeze(self):\n        \"\"\"Freeze BERT.\"\"\"\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n# Test function\ndef test_text_encoder():\n    # Create text encoder\n    text_encoder = TextEncoder(embedding_dim=256)\n    print(f\"Created text encoder on device: {text_encoder.device}\")\n    \n    # Test with some class names\n    class_names = [\n        \"Healthy grass\",\n        \"Stressed grass\",\n        \"Artificial turf\"\n    ]\n    \n    # Test single class encoding\n    single_embedding = text_encoder.encode_class_description(class_names[0])\n    print(f\"\\nSingle class embedding shape: {single_embedding.shape}\")\n    \n    # Test batch encoding\n    batch_embeddings = text_encoder.encode_batch(class_names)\n    print(f\"Batch embeddings shape: {batch_embeddings.shape}\")\n    \n    # Compare embeddings of similar and different classes\n    embeddings = {\n        name: text_encoder.encode_class_description(name) \n        for name in [\"Healthy grass\", \"Stressed grass\", \"Water\"]\n    }\n    \n    # Calculate cosine similarities\n    print(\"\\nCosine similarities:\")\n    for name1 in embeddings:\n        for name2 in embeddings:\n            sim = torch.nn.functional.cosine_similarity(\n                embeddings[name1], \n                embeddings[name2]\n            ).item()\n            print(f\"{name1} vs {name2}: {sim:.3f}\")\n\n# Run the test\ntest_text_encoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:44:28.323209Z","iopub.execute_input":"2025-07-28T07:44:28.323521Z","iopub.status.idle":"2025-07-28T07:44:57.341149Z","shell.execute_reply.started":"2025-07-28T07:44:28.323499Z","shell.execute_reply":"2025-07-28T07:44:57.340424Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d560c167cf174dd0a6c5ae11e94693f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9970ac26ca1412e9b2b7d9ccd493a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f0cdb87e2a463eb1d0fa980842148c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462b3381b7044ad58572509cb37688c4"}},"metadata":{}},{"name":"stderr","text":"2025-07-28 07:44:43.371082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753688683.501898      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753688683.537894      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc88181ab8148afae96befff0467635"}},"metadata":{}},{"name":"stdout","text":"Created text encoder on device: cuda\n\nSingle class embedding shape: torch.Size([1, 256])\nBatch embeddings shape: torch.Size([3, 256])\n\nCosine similarities:\nHealthy grass vs Healthy grass: 1.000\nHealthy grass vs Stressed grass: 0.897\nHealthy grass vs Water: 0.827\nStressed grass vs Healthy grass: 0.897\nStressed grass vs Stressed grass: 1.000\nStressed grass vs Water: 0.873\nWater vs Healthy grass: 0.827\nWater vs Stressed grass: 0.873\nWater vs Water: 1.000\n","output_type":"stream"}],"execution_count":11}]}